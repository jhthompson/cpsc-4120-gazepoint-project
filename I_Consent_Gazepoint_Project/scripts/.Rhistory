getOption("repos")
options(repos = c(CRAN = "https://cran.rstudio.com"))
install.packages(c("backports", "caTools", "digest", "glue", "markdown", "mime", "Rcpp", "stringi", "yaml"))
library(ggplot2)
rm(list = ls())
library(ggplot2)
library(ggplot2)
data <- read.csv("combined_tweets.csv")
getwd()
switch_wd
install.packages('devtools')
devtools::install_github('fawda123/rStrava')
athl_fun(2837007, trace = FALSE)
CMD INSTALL
install.packages(rStrava)
install.packages(rstrahelp.search('notoken', package = 'rStrava'))
help.search('notoken', package = 'rStrava')
?athl_fun
library(rStrava)
athl_fun(2837007, trace = FALSE)
athl_fun(7780234, trace = FALSE)
athl_fun(21953458, trace=FALSE)
test <- 21953458
test <- athl_fun(21953458, trace=FALSE)
View(test)
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
N <- nrow(Default)
train <- sample(N, N/2)
glm.fit <- glm(default ~ balance + income, family=binomial, subset=train)
pred.probs <- predict(glm.fit, Default[-train], type="response")
View(pred.probs)
pred.default <- rep("No", N/2)
pred.default[pred.probs > 0.5] <- "Yes"
error.rate <- mean(pred.default != default[-train])
error.rate
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
N <- nrow(Default)
train <- sample(N, N/2)
glm.fit <- glm(default ~ balance + income, family=binomial, subset=train)
pred.probs <- predict(glm.fit, Default[-train], type="response")
View(pred.probs)
pred.default <- rep("No", N/2)
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
N <- nrow(Default)
train <- sample(N, N/2)
glm.fit <- glm(default ~ balance + income, family = binomial, subset = train)
pred.probs <- predict(glm.fit, Default[-train], type="response")
pred.default <- rep("No", N/2)
pred.default[pred.probs > 0.5] <- "Yes"
error.rate <- mean(pred.default != default[-train])
error.rate
pred.deault
pred.default
summary(pred.defaultault)
summary(pred.default)
train <- sample(N, N / 2)
pred.default <- rep("No", N / 2)
pred.default[pred.probs > 0.5] <- "Yes"
error.rate <- mean(pred.default != default[-train])
error.rate
library(ISLR)
attach(Default)
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
N <- nrow(Default)
train <- sample(N, N / 2)
glm.fit <- glm(default ~ balance + income, family = binomial, subset = train)
pred.probs <- predict(glm.fit, Default[-train], type="response")
pred.default <- rep("No", N / 2)
pred.default[pred.probs > 0.5] <- "Yes"
error.rate <- mean(pred.default != default[-train])
error.rate <- mean(pred.default != default[-train])
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
N <- nrow(Default)
?sample
train <- sample(N, N/2)
glm.fit <- glm(default ~ balance + income, family = binomial, subset = train)
summary(glm)
summary(glm.fit)
pred.probs <- predict(glm.fit, Default[-train], type="response")
pred.default <- rep("No", N/2)
pred.default[pred.probs > 0.5] <- "Yes"
error.rate <- mean(pred.default != default[-train])
pred.default[pred.probs > 0.9] <- "Yes"
error.rate <- mean(pred.default != default[-train])
error.rate
error.rate <- mean(pred.default != default)
error.rate
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
N <- nrow(Default)
train <- sample(N, N/2)
glm.fit <- glm(default ~ balance + income, family = binomial, subset = train)
pred.probs <- predict(glm.fit, Default[-train], type="response")
pred.default <- rep("No", N/2)
pred.default[pred.probs > 0.5] <- "Yes"
error.rate <- mean(pred.default != pred.default[-train])
library(ISLR)
attach(Default)
summary(Default)
set.seed(42)
N <- nrow(Default)
train <- sample(N, N/2)
glm.fit <- glm(default ~ balance + income, family = binomial, subset = train)
View(Default)
View(Default[-train,1])
# View(Default)
# View(Default[-train,1]) # means only select rows that are NOT in the training set, and the
# comma *IMPORTANT* indicates to select all columns
pred.probs <- predict(glm.fit, Default[-train,0,2:3], type="response")
# View(Default)
# View(Default[-train,1]) # means only select rows that are NOT in the training set, and the
# comma *IMPORTANT* indicates to select all columns
pred.probs <- predict(glm.fit, Default[-train,], type="response")
# View(Default)
# View(Default[-train,1]) # means only select rows that are NOT in the training set, and the
# comma *IMPORTANT* indicates to select all columns
pred.probs <- predict(glm.fit, Default[-train,c(1,3,4)], type="response")
# View(Default)
# View(Default[-train,1]) # means only select rows that are NOT in the training set, and the
# comma *IMPORTANT* indicates to select all columns
# same thing as c(1,3,4), which leaves out the student column, since we didn't use that to
# train the model at all
pred.probs1 <- predict(glm.fit, Default[-train,c(1,3,4)], type="response")
pred.probs <- predict(glm.fit, Default[-train,], type="response")
pred.probs == pred.probs1
?eq
?equal
summary(pred.probs)
summary(pred.probs1)
library(boot)
?cv.glm
?abs
# train model on all data
glm.fit <- glm(default ~ balance + income, family=binomial)
my.cost <- function(r, pi = 0) {
mean(abs(r-pi) > 0.5)
}
# train model on all data
glm.fit <- glm(default ~ balance + income, family=binomial)
?cv.glm
cv.error.10 <- cv.glm(Default, glm.fit, my.cost, K=10)
cv.error.10$delta
file.path(R.home("bin"), "R")
library(MASS)
library(tree)
install.packages('tree')
library(MASS)
library(tree)
set.seed(1)
N <- nrow(Boston)
train <- sample(1:N, N/2)
test <- seq(1:N, i)
rm(list = ls())
set.seed(1)
N <- nrow(Boston)
train <- sample(1:N, N/2)
test <- seq(1:N, i)
ls
ls()
library(MASS)
library(tree)
rm(list = ls())
set.seed(1)
N <- nrow(Boston)
train <- sample(1:N, N/2)
test <- seq(1:N, i)
test <- seq(1:N)[-train]
tree.boston <- tree(medv~., Boston, subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston)
summary(Boston)
library(MASS)
library(tree)
rm(list = ls())
set.seed(1)
N <- nrow(Boston)
train <- sample(1:N, N/2)
test <- seq(1:N)[-train]
tree.boston <- tree(medv~., Boston, subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston)
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type=’b’)
plot(cv.boston$size, cv.boston$dev, type='b')
prune.boston <- prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston)
yhat.tree <- predict(tree.boston, Boston[test,])
boston.test <- Boston[test, "medv"]
plot(yhat.tree, boston.test)
abline(0,1)
mean((yhat.tree-boston.test)^2)
lm.boston <- lm(medv~ ., Boston, subset=train)
yhat.lm <- predict(lm.boston, Boston[test,])
boston.test <- Boston[test, "medv"]
mean((yhat.lm-boston.test)^2)
library(MASS)
library(tree)
library(MASS)
library(tree)
rm(list = ls())
set.seed(1)
library(MASS)
library(tree)
#rm(list = ls())
set.seed(1)
N <- nrow(Boston)
train <- sample(1:N, N/2)
test <- seq(1:N)[-train]
tree.boston <- tree(medv~., Boston, subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston)
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
prune.boston <- prune.tree(tree.boston, best=5)
plot(prune.boston)
yhat.tree <- predict(tree.boston, Boston[test,])
text(prune.boston)
boston.test <- Boston[test, "medv"]
plot(yhat.tree, boston.test)
abline(0,1)
mean((yhat.tree-boston.test)^2)
lm.boston <- lm(medv~ ., Boston, subset=train)
yhat.lm <- predict(lm.boston, Boston[test,])
boston.test <- Boston[test, "medv"]
mean((yhat.lm-boston.test)^2)
library(randomForest)
install.packages('randomForest')
set.seed(1)
library(randomForest)
set.seed(1)
P <- ncol(Boston)-1
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=P, importance
=TRUE)
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=P, importance=TRUE)
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=P, importance=TRUE)
bag.boston
# how well did bagging perform
yhat.bag <- predict(bag.boston, Boston[test,])
mean((yhat.bag-boston.test)^2)
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=P, ntree=25)
yhat.bag <- predict(bag.boston, Boston[test,])
mean((yhat.bag-boston.test)^2)
rf.boston <- randomForest(medv~., data=Boston, subset=train, ry=6, importance=TRUE)
yhat.rf <- predict(rf.boston, Boston[test,])
mean((yhat.rf-boston.test)^2)
rf.boston <- randomForest(medv~., data=Boston, subset=train, mtry=6, importance=TRUE)
yhat.rf <- predict(rf.boston, Boston[test,])
mean((yhat.rf-boston.test)^2)
rf.boston <- randomForest(medv~., data=Boston, subset=train, mtry=6, importance=TRUE)
yhat.rf <- predict(rf.boston, Boston[test,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
install.packages('rbm')
install.packages('r=gbm')
install.packages('gbm')
set.seed(1)
boost.boston <- gbm(medv~., data=Boston[train,], distribution="gaussian", n.trees
=5000, interaction.depth=4)
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~., data=Boston[train,], distribution="gaussian", n.trees
=5000, interaction.depth=4)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
yhat.boost <- predict(boost.boston, newdata=Boston[test,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
boost.boston <- gbm(medv~., data=Boston[train,], distribution="gaussian", n.trees
=5000, interaction.depth=4, shrinkage=0.2, verbose=F)
yhat.boost <- predict(boost.boston, Boston[test,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
library(e1071)
install.packages('e1071')
rm(list-ls())
rm(list=ls())
set.seed(1)
set.seed(1)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
plot(x, col=(3-y))
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
library(e1071)
svmfit <- svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
plot(svmfit , dat)
svmfit$index
summary(svmfit)
svmfit <- svm(y~., data=dat, kernel="linear", cost=0.1, scale=FALSE)
plot(svmfit,dat)
svmfit$index
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel="linear", ranges=list(cost=c(0.001,                                                                           0.01, 0.1, 1,5, 10, 100)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
xtest <- matrix(rnorm(20*2), ncol=2)
ytest <- sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,] <- xtest[ytest==1,] + 1
testdat <- data.frame(x=xtest, y=as.factor(ytest))
ypred <- predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
# generate some data with a nonlinear class boundary
set.seed(1)
x <- matrix(rnorm(200*2), ncol=2)
x[1:100,] <- x[1:100,] + 2
x[101:150,] <- x[101:150,] - 2
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x, y=as.factor(y))
# plotting data makes it clear that class boundary is indeed non-linear
plot(x, col=y)
# data is randomly split into training and testing groups
train <- sample(200, 100)
test <- seq(1:200)[-train]
# now fit the training data using the svm() function with a radial kernel
svmfit <- sym(y~., data=dat[train,], kernel="radial", gamma=1, cost=1)
# now fit the training data using the svm() function with a radial kernel
svmfit <- svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1)
summary(svmfit)
plot(svmfit, dat[train,])
#increase the value of cost => reduce training errors, but makes more irregular
# decision boundary that seems to be at risk of overfitting
svmfit <- svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1e5)
plot(svmfit, dat[train,])
set.seed(1)
tune.out <- tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c
(0.1, 1, 10, 100, 1000), gamma=c(0.5, 1, 2, 3, 4)))
summary(tune.out)
table(true=dat[test, "y"], pred=predict(tune.out$best.model, newdata=dat[test,]))
rm(list=ls())
rm(list=ls())
rm(list=ls())
# >> general preparations -------------------------------------------------------
source('customFunc.R')
setwd("C:/Users/jerem/Google Drive (jht2@g.clemson.edu)/Eye Tracking Methodology and Applications - CPSC 4120/faces/faces/src/tutorial_etra18")
# >> general preparations -------------------------------------------------------
source('customFunc.R')
load.libraries(c('afex','sciplot','knitr'))
pdf.options(family="NimbusSan",useDingbats=FALSE)
df <- read.csv("amfo.csv") # open data
head(df)
str(df)
d1 <- aggregate(df[ ,9], by = list(df$subj, df$ttype, df$block, df$trial), FUN = mean)
data <- read.table("./data/c0_User 1_calibration-verification-image-amfo.dat", sep = "")
rm(list=ls())
setwd("~/GitHub/cpsc-4120-gazepoint-project/I_Consent_Gazepoint_Project/scripts")
data <- read.table("./data/c0_User 1_calibration-verification-image-amfo.dat", sep = "")
?read.table
data <- read.table("./data/c0_User 1_calibration-verification-image-amfo.dat", sep = "", col.names = c("time", 'K'))
View(data)
View(data)
remotes::install_github("DavisVaughan/slide")
install.packages("remote")
remotes::install_github("DavisVaughan/slide")
install.packages("remotes")
library("remotes")
install.packages("remotes")
remotes::install_github("DavisVaughan/slide")
library(slide)
remotes::install_github("DavisVaughan/slide")
find_rtools
find_rtools()
